---
layout: post
export_on_save:
    html: true
title: Classifation
author: Luke Anglin
image: https://www.gstatic.com/education/formulas/images_long_sheet/en/bayes__theorem.svg
description: Looking at different classifation algorithms, and the theorems and math behind them
topics: Bayes Theorem, SGD, iterative and other approaches to learning
sources: https://towardsdatascience.com/probability-learning-ii-how-bayes-theorem-is-applied-in-machine-learning-bd747a960962
publish: 
---

# Classification

I think all of this can be better understood by **doing**.  See the relevant notebooks below. 

The relevant [notebook](http://localhost:8888/notebooks/Data-Science/MLProjects/Notes/Classification%20-%20Happiness%20Score.ipynb) can be found here.  These are my personal notes.  

The [original](http://localhost:8888/notebooks/Data-Science/Akramz/03.Classification.ipynb) can be found here.

These notebooks cover: 

* Binary classifiers
* SGD Classifier in detail
* Classifier performance evaluation
* Multiclass Classification
    * One vs. Rest (OVR)
    * One vs. One (OVO)
* Muti-label Classification
    * K-Nearest Neighbors






